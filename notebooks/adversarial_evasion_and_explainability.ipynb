{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c9773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. SETUP ATTACK PARAMETERS\n",
    "# Epsilon (Îµ) is the noise strength. 0 means no change. 0.3 is visible noise.\n",
    "epsilons = [0, 0.05, 0.1, 0.15, 0.2, 0.3]\n",
    "\n",
    "# Mean and Std from Member 1 (used to bring image back to normal colors for display)\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "def denormalize(tensor):\n",
    "    \"\"\"Converts a normalized tensor back into a viewable RGB image.\"\"\"\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m) # Multiply by std and add mean (reverse of normalization)\n",
    "    return torch.clamp(tensor, 0, 1) # Ensure pixels stay in valid [0, 1] range\n",
    "\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    \"\"\"The Fast Gradient Sign Method (FGSM) logic.\"\"\"\n",
    "    # Find the direction of the gradient (which pixels 'hurt' the model's accuracy)\n",
    "    sign_data_grad = data_grad.sign()\n",
    "\n",
    "    # Create the perturbed image by moving pixels in that 'harmful' direction\n",
    "    perturbed_image = image + epsilon * sign_data_grad\n",
    "\n",
    "    # Keep the image math consistent with its original normalized range\n",
    "    perturbed_image = torch.clamp(perturbed_image, image.min(), image.max())\n",
    "    return perturbed_image\n",
    "\n",
    "# 2. SELECT A \"VICTIM\" IMAGE\n",
    "# We need a Fake image that the model CURRENTLY predicts correctly as Fake.\n",
    "model.eval() # Put model in evaluation mode\n",
    "found = False\n",
    "\n",
    "for imgs, lbls in test_loader:\n",
    "    imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "    output = model(imgs) # Run image through model\n",
    "    _, pred = output.max(1) # Get the current prediction\n",
    "\n",
    "    for i in range(len(lbls)):\n",
    "        # Check if Actual is FAKE (0) and Model also says FAKE (0)\n",
    "        if lbls[i].item() == 0 and pred[i].item() == 0:\n",
    "            # .detach().clone() creates a fresh 'leaf' variable to avoid PyTorch errors\n",
    "            target_image = imgs[i].unsqueeze(0).detach().clone()\n",
    "            target_label = lbls[i].unsqueeze(0)\n",
    "            found = True\n",
    "            break\n",
    "    if found: break\n",
    "\n",
    "# 3. RUN THE ATTACK TRAJECTORY\n",
    "# This loop applies increasing levels of noise to see when the model \"breaks\"\n",
    "fig, axes = plt.subplots(1, len(epsilons), figsize=(18, 5))\n",
    "\n",
    "for i, eps in enumerate(epsilons):\n",
    "    # Prepare a fresh copy of the image and tell PyTorch to track pixel math (gradients)\n",
    "    temp_img = target_image.clone().detach()\n",
    "    temp_img.requires_grad = True\n",
    "\n",
    "    # FORWARD PASS: Get the model's prediction\n",
    "    output = model(temp_img)\n",
    "    loss = F.cross_entropy(output, target_label) # Calculate how 'correct' it is\n",
    "\n",
    "    # BACKWARD PASS: Calculate the 'Gradient' (which pixels to change)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # APPLY ATTACK: Create the noisy adversarial image\n",
    "    adv_image = fgsm_attack(temp_img, eps, temp_img.grad.data)\n",
    "\n",
    "    # TEST THE ATTACK: See if the model is fooled\n",
    "    with torch.no_grad():\n",
    "        adv_out = model(adv_image)\n",
    "        adv_pred = adv_out.argmax(1).item() # The new (hopefully wrong) prediction\n",
    "        conf = F.softmax(adv_out, dim=1).max().item() # Confidence in that prediction\n",
    "\n",
    "    # VISUALIZATION: Show the results\n",
    "    img_show = denormalize(adv_image.squeeze().cpu().detach()) # Prep for display\n",
    "    axes[i].imshow(img_show.permute(1, 2, 0)) # Change (C, H, W) to (H, W, C) for Matplotlib\n",
    "\n",
    "    res_label = \"REAL\" if adv_pred == 1 else \"FAKE\"\n",
    "    # SUCCESS: If prediction is REAL (1), title turns RED to show the model was FOOLED\n",
    "    title_color = 'red' if adv_pred == 1 else 'black'\n",
    "\n",
    "    axes[i].set_title(f\"Eps: {eps}\\nPred: {res_label}\\nConf: {conf:.2f}\", color=title_color)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(\"Adversarial Evasion Analysis (fooling the AI)\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# ==============================================================\n",
    "# ANALYSIS & GRAD-CAM COMPARISON (FIXED)\n",
    "# ==============================================================\n",
    "import cv2\n",
    "\n",
    "# 1. APPLY BLUR (Minimal Modification Task)\n",
    "# Re-scaling to 0-255 for OpenCV operations\n",
    "adv_img_255 = (denormalize(adv_image.squeeze().cpu().detach()).permute(1,2,0).numpy() * 255).astype(np.uint8)\n",
    "blurred_adv = cv2.GaussianBlur(adv_img_255, (3, 3), 0)\n",
    "\n",
    "# 2. GRAD-CAM COMPARISON FUNCTION\n",
    "def get_gradcam_heatmap(input_tensor):\n",
    "    # .detach().clone() creates a fresh copy to avoid memory graph issues\n",
    "    fresh_input = input_tensor.detach().clone()\n",
    "    fresh_input.requires_grad = True\n",
    "\n",
    "    model.zero_grad()\n",
    "    output = model(fresh_input)\n",
    "\n",
    "    # We use .backward() here to get gradients for the heatmap\n",
    "    output.max(1)[0].backward()\n",
    "\n",
    "    # 'features' is updated via the hook from Member 2's code\n",
    "    heatmap = torch.mean(features, dim=1)[0].cpu().detach().numpy()\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= (np.max(heatmap) + 1e-8)\n",
    "    return cv2.resize(heatmap, (32, 32))\n",
    "\n",
    "# Generate Heatmaps using the fixed function\n",
    "# We use .detach() to ensure we aren't carrying old math history\n",
    "orig_heatmap = get_gradcam_heatmap(target_image.detach())\n",
    "adv_heatmap = get_gradcam_heatmap(adv_image.detach())\n",
    "\n",
    "# 3. FINAL VISUAL COMPARISON\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot 1: Original image with focus\n",
    "img_orig = denormalize(target_image.squeeze().cpu().detach()).permute(1,2,0)\n",
    "ax[0].imshow(img_orig)\n",
    "ax[0].imshow(orig_heatmap, cmap='jet', alpha=0.4)\n",
    "ax[0].set_title(\"Original (FAKE)\\nGrad-CAM focus on object\")\n",
    "ax[0].axis('off')\n",
    "\n",
    "# Plot 2: Attacked image with focus shift\n",
    "# Note: img_show comes from your previous attack cell\n",
    "ax[1].imshow(img_show.permute(1, 2, 0))\n",
    "ax[1].imshow(adv_heatmap, cmap='jet', alpha=0.4)\n",
    "ax[1].set_title(\"Adversarial (PREDICTED REAL)\\nGrad-CAM focus is scattered\")\n",
    "ax[1].axis('off')\n",
    "\n",
    "# Plot 3: Minimal Modification (Blur)\n",
    "ax[2].imshow(blurred_adv)\n",
    "ax[2].set_title(\"Minimal Modification\\n(Gaussian Blur applied)\")\n",
    "ax[2].axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Task Completed: Original vs Modified images displayed with Grad-CAM shift.\")\n",
    "\n",
    "# ==============================\n",
    "# PHASE 2 NUMERICAL EVALUATION\n",
    "# ==============================\n",
    "def evaluate_adversarial_accuracy(model, loader, epsilon=0.1):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    flipped_to_real = 0\n",
    "    fake_count = 0\n",
    "\n",
    "    print(f\"Evaluating accuracy under attack (Epsilon: {epsilon})...\")\n",
    "\n",
    "    for imgs, lbls in tqdm(loader):\n",
    "        imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "        imgs.requires_grad = True\n",
    "\n",
    "        # 1. Forward pass to get gradients\n",
    "        outputs = model(imgs)\n",
    "        loss = F.cross_entropy(outputs, lbls)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # 2. Apply FGSM Attack\n",
    "        adv_imgs = fgsm_attack(imgs, epsilon, imgs.grad.data)\n",
    "\n",
    "        # 3. Predict on Adversarial Images\n",
    "        with torch.no_grad():\n",
    "            adv_outputs = model(adv_imgs)\n",
    "            _, preds = adv_outputs.max(1)\n",
    "\n",
    "            total += lbls.size(0)\n",
    "            correct += (preds == lbls).sum().item()\n",
    "\n",
    "            # Count specifically how many Fakes (0) were predicted as Real (1)\n",
    "            fakes = (lbls == 0)\n",
    "            fake_count += fakes.sum().item()\n",
    "            flipped_to_real += ((preds == 1) & fakes).sum().item()\n",
    "\n",
    "        if total >= 1000: break # Stop after 1000 images for speed\n",
    "\n",
    "    final_acc = (correct / total) * 100\n",
    "    evasion_rate = (flipped_to_real / fake_count) * 100\n",
    "\n",
    "    print(f\"\\n--- PHASE 2 RESULTS ---\")\n",
    "    print(f\"Adversarial Accuracy: {final_acc:.2f}%\")\n",
    "    print(f\"Evasion Success Rate (Fakes predicted as Real): {evasion_rate:.2f}%\")\n",
    "    return final_acc\n",
    "\n",
    "# Run evaluation for Epsilon 0.1\n",
    "phase2_acc = evaluate_adversarial_accuracy(model, test_loader, epsilon=0.1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
